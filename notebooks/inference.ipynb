{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Alpamayo-R1 Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook will load some example data from the NVIDIA [PhysicalAI-AV Dataset](https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles) and run the Alpamayo-R1 model on it, producing and visualizing output trajectories and associated reasoning traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import mediapy as mp\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from alpamayo_r1.models.alpamayo_r1 import AlpamayoR1\n",
    "from alpamayo_r1.load_physical_aiavdataset import load_physical_aiavdataset\n",
    "from alpamayo_r1 import helper\n",
    "from transformers import BitsAndBytesConfig  # <--- 이 줄 추가\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Load model and construct data preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc68a6388c5b4600b6804a59a2bfcf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = AlpamayoR1.from_pretrained(\"nvidia/Alpamayo-R1-10B\", dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16  # 연산은 float16으로 수행하여 충돌 방지\n",
    ")\n",
    "\n",
    "model = AlpamayoR1.from_pretrained(\n",
    "    \"nvidia/Alpamayo-R1-10B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = helper.get_processor(model.tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq length: torch.Size([1, 3006])\n"
     ]
    }
   ],
   "source": [
    "clip_ids = pd.read_parquet(\"clip_ids.parquet\")[\"clip_id\"].tolist()\n",
    "clip_id = clip_ids[774]\n",
    "# clip_id = '030c760c-ae38-49aa-9ad8-f5650a545d26'\n",
    "\n",
    "data = load_physical_aiavdataset(clip_id)\n",
    "\n",
    "messages = helper.create_message(data[\"image_frames\"].flatten(0, 1))\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(\"seq length:\", inputs.input_ids.shape)\n",
    "model_inputs = {\n",
    "    \"tokenized_data\": inputs,\n",
    "    \"ego_history_xyz\": data[\"ego_history_xyz\"],\n",
    "    \"ego_history_rot\": data[\"ego_history_rot\"],\n",
    "}\n",
    "model_inputs = helper.to_device(model_inputs, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cusolver error: CUSOLVER_STATUS_INTERNAL_ERROR, when calling `cusolverDnCreate(handle)`. If you keep seeing this error, you may use `torch.backends.cuda.preferred_linalg_library()` to try linear algebra operators with other supported backends. See https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.preferred_linalg_library",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m torch.cuda.manual_seed_all(\u001b[32m42\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, dtype=torch.bfloat16):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     pred_xyz, pred_rot, extra = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_trajectories_from_data_with_vlm_rollout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.98\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_traj_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Feel free to raise this for more output trajectories and CoC traces.\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_generation_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_extra\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# the size is [batch_size, num_traj_sets, num_traj_samples]\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChain-of-Causation (per trajectory):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, extra[\u001b[33m\"\u001b[39m\u001b[33mcot\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vv/alpamayo/src/alpamayo_r1/models/alpamayo_r1.py:307\u001b[39m, in \u001b[36mAlpamayoR1.sample_trajectories_from_data_with_vlm_rollout\u001b[39m\u001b[34m(self, data, top_p, top_k, temperature, num_traj_samples, num_traj_sets, diffusion_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    300\u001b[39m hist_xyz_rep = einops.repeat(\n\u001b[32m    301\u001b[39m     ego_history_xyz[:, -\u001b[32m1\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mb ... -> (b n) ...\u001b[39m\u001b[33m\"\u001b[39m, n=n_samples_total\n\u001b[32m    302\u001b[39m )\n\u001b[32m    303\u001b[39m hist_rot_rep = einops.repeat(\n\u001b[32m    304\u001b[39m     ego_history_rot[:, -\u001b[32m1\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mb ... -> (b n) ...\u001b[39m\u001b[33m\"\u001b[39m, n=n_samples_total\n\u001b[32m    305\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m pred_xyz, pred_rot = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m.\u001b[49m\u001b[43maction_to_traj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampled_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhist_xyz_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhist_rot_rep\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# 4) Reshape to (B, num_traj_samples, n_traj, ...)\u001b[39;00m\n\u001b[32m    312\u001b[39m pred_xyz = einops.rearrange(\n\u001b[32m    313\u001b[39m     pred_xyz, \u001b[33m\"\u001b[39m\u001b[33m(b ns nj) ... -> b ns nj ...\u001b[39m\u001b[33m\"\u001b[39m, ns=num_traj_sets, nj=num_traj_samples\n\u001b[32m    314\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vv/alpamayo/src/alpamayo_r1/action_space/unicycle_accel_curvature.py:329\u001b[39m, in \u001b[36mUnicycleAccelCurvatureActionSpace.action_to_traj\u001b[39m\u001b[34m(self, action, traj_history_xyz, traj_history_rot, t0_states)\u001b[39m\n\u001b[32m    326\u001b[39m kappa = kappa * kappa_std + kappa_mean\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t0_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     t0_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimate_t0_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraj_history_xyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraj_history_rot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m v0 = t0_states[\u001b[33m\"\u001b[39m\u001b[33mv\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    332\u001b[39m dt = \u001b[38;5;28mself\u001b[39m.dt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vv/alpamayo/ar1_venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vv/alpamayo/ar1_venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vv/alpamayo/src/alpamayo_r1/action_space/unicycle_accel_curvature.py:218\u001b[39m, in \u001b[36mUnicycleAccelCurvatureActionSpace.estimate_t0_states\u001b[39m\u001b[34m(self, traj_history_xyz, traj_history_rot)\u001b[39m\n\u001b[32m    215\u001b[39m theta = so3_to_yaw_torch(traj_history_rot)\n\u001b[32m    216\u001b[39m theta = unwrap_angle(theta)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m v = \u001b[43mdxy_theta_to_v_without_v0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_lambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_ridge\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_ridge\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (..., N+1)\u001b[39;00m\n\u001b[32m    221\u001b[39m v_t0 = v[..., -\u001b[32m1\u001b[39m]\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mv\u001b[39m\u001b[33m\"\u001b[39m: v_t0}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vv/alpamayo/ar1_venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vv/alpamayo/ar1_venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vv/alpamayo/ar1_venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vv/alpamayo/src/alpamayo_r1/action_space/utils.py:395\u001b[39m, in \u001b[36mdxy_theta_to_v_without_v0\u001b[39m\u001b[34m(dxy, theta, dt, v_lambda, v_ridge)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;66;03m# strip off the x_init term\u001b[39;00m\n\u001b[32m    393\u001b[39m lhs = ATA + DTD + ridge_term\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m L = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m y = torch.cholesky_solve(rhs.unsqueeze(-\u001b[32m1\u001b[39m), L).squeeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (..., N+1)\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[31mRuntimeError\u001b[39m: cusolver error: CUSOLVER_STATUS_INTERNAL_ERROR, when calling `cusolverDnCreate(handle)`. If you keep seeing this error, you may use `torch.backends.cuda.preferred_linalg_library()` to try linear algebra operators with other supported backends. See https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.preferred_linalg_library"
     ]
    }
   ],
   "source": [
    "torch.cuda.manual_seed_all(42)\n",
    "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    pred_xyz, pred_rot, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "        data=copy.deepcopy(model_inputs),\n",
    "        top_p=0.98,\n",
    "        temperature=0.6,\n",
    "        num_traj_samples=3,  # Feel free to raise this for more output trajectories and CoC traces.\n",
    "        max_generation_length=256,\n",
    "        return_extra=True,\n",
    "    )\n",
    "\n",
    "# the size is [batch_size, num_traj_sets, num_traj_samples]\n",
    "print(\"Chain-of-Causation (per trajectory):\\n\", extra[\"cot\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Visualizing data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.show_images(data[\"image_frames\"].flatten(0, 1).permute(0, 2, 3, 1), columns=4, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def rotate_90cc(xy):\n",
    "    # Rotate (x, y) by 90 deg CCW -> (y, -x)\n",
    "    return np.stack([-xy[1], xy[0]], axis=0)\n",
    "\n",
    "\n",
    "for i in range(pred_xyz.shape[2]):\n",
    "    pred_xy = pred_xyz.cpu()[0, 0, i, :, :2].T.numpy()\n",
    "    pred_xy_rot = rotate_90cc(pred_xy)\n",
    "    gt_xy = data[\"ego_future_xyz\"].cpu()[0, 0, :, :2].T.numpy()\n",
    "    gt_xy_rot = rotate_90cc(gt_xy)\n",
    "    plt.plot(*pred_xy_rot, \"o-\", label=f\"Predicted Trajectory #{i + 1}\")\n",
    "plt.ylabel(\"y coordinate (meters)\")\n",
    "plt.xlabel(\"x coordinate (meters)\")\n",
    "plt.plot(*gt_xy_rot, \"r-\", label=\"Ground Truth Trajectory\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xy = pred_xyz.cpu().numpy()[0, 0, :, :, :2].transpose(0, 2, 1)\n",
    "diff = np.linalg.norm(pred_xy - gt_xy[None, ...], axis=1).mean(-1)\n",
    "print(\"minADE:\", diff.min(), \"meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204266f-a0e4-4287-897f-79597f5fc17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
